{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: The Quiet Revolution in Python Numerical Computing\n",
    "\n",
    "**Why PyTorch isn't just for MLâ€”it's the future of accelerated math in Python**\n",
    "\n",
    "---\n",
    "\n",
    "NumPy has been the backbone of numerical Python for decades. But there's a shift happening that few are talking about: **PyTorch is becoming a superior general-purpose numerical computing library**â€”not just for machine learning.\n",
    "\n",
    "### The key insight:\n",
    "When you write your numerical code in PyTorch, you get:\n",
    "1. **GPU acceleration for free** â€” move your tensors to GPU with `.to('cuda')` and your existing code runs on accelerators\n",
    "2. **Automatic differentiation** â€” gradients without manual derivation\n",
    "3. **Future-proof compatibility** â€” as PyTorch improves (better kernels, new hardware support), your code gets faster without changes\n",
    "4. **ML-ready data** â€” your computations are already in tensors, ready for model integration\n",
    "\n",
    "This notebook benchmarks PyTorch against NumPy across common numerical operations to demonstrate the performance characteristics and the \"write once, accelerate anywhere\" paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM CONFIGURATION\n",
      "============================================================\n",
      "NumPy version: 2.1.2\n",
      "PyTorch version: 2.8.0+cu129\n",
      "CUDA available: True\n",
      "CUDA version: 12.9\n",
      "GPU: NVIDIA RTX PRO 6000 Blackwell Workstation Edition\n",
      "GPU Memory: 101.9 GB\n",
      "MPS available (Apple Silicon): False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check available devices\n",
    "print(\"=\"*60)\n",
    "print(\"SYSTEM CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"MPS available (Apple Silicon): {torch.backends.mps.is_available()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on devices: ['cpu', 'cuda']\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "CPU = 'cpu'\n",
    "GPU = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else None\n",
    "\n",
    "DEVICES = [CPU]\n",
    "if GPU:\n",
    "    DEVICES.append(GPU)\n",
    "    \n",
    "print(f\"Benchmarking on devices: {DEVICES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(func: Callable, n_warmup: int = 3, n_runs: int = 10, sync_cuda: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark a function with proper warmup and CUDA synchronization.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to benchmark (should return None or a tensor)\n",
    "        n_warmup: Number of warmup iterations\n",
    "        n_runs: Number of timed iterations\n",
    "        sync_cuda: Whether to synchronize CUDA before timing\n",
    "    \n",
    "    Returns:\n",
    "        Dict with timing statistics\n",
    "    \"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(n_warmup):\n",
    "        func()\n",
    "    \n",
    "    # Synchronize before timing\n",
    "    if sync_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        func()\n",
    "        \n",
    "        # Synchronize after each run for accurate GPU timing\n",
    "        if sync_cuda and torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(times) * 1000,  # Convert to ms\n",
    "        'std': np.std(times) * 1000,\n",
    "        'min': np.min(times) * 1000,\n",
    "        'max': np.max(times) * 1000,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_speedup(baseline: float, comparison: float) -> str:\n",
    "    \"\"\"Format speedup as a string with color indication.\"\"\"\n",
    "    if comparison < baseline:\n",
    "        speedup = baseline / comparison\n",
    "        return f\"ğŸš€ {speedup:.1f}x faster\"\n",
    "    else:\n",
    "        slowdown = comparison / baseline\n",
    "        return f\"ğŸ¢ {slowdown:.1f}x slower\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 1: Matrix Operations\n",
    "\n",
    "Matrix multiplication is the bread and butter of numerical computing. Let's see how NumPy compares to PyTorch on CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 1: Matrix Multiplication (A @ B)\n",
      "============================================================\n",
      "\n",
      "Matrix size: 256x256\n",
      "  NumPy:         0.11 Â± 0.11 ms\n",
      "  PyTorch (CPU): 0.06 Â± 0.00 ms ğŸš€ 1.7x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 7.0x faster\n",
      "\n",
      "Matrix size: 512x512\n",
      "  NumPy:         0.32 Â± 0.01 ms\n",
      "  PyTorch (CPU): 2.99 Â± 4.69 ms ğŸ¢ 9.4x slower\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 15.9x faster\n",
      "\n",
      "Matrix size: 1024x1024\n",
      "  NumPy:         2.61 Â± 4.82 ms\n",
      "  PyTorch (CPU): 3.87 Â± 4.30 ms ğŸ¢ 1.5x slower\n",
      "  PyTorch (GPU): 0.04 Â± 0.00 ms ğŸš€ 59.0x faster\n",
      "\n",
      "Matrix size: 2048x2048\n",
      "  NumPy:         5.88 Â± 0.40 ms\n",
      "  PyTorch (CPU): 9.38 Â± 0.07 ms ğŸ¢ 1.6x slower\n",
      "  PyTorch (GPU): 0.24 Â± 0.02 ms ğŸš€ 24.2x faster\n",
      "\n",
      "Matrix size: 4096x4096\n",
      "  NumPy:         46.70 Â± 0.16 ms\n",
      "  PyTorch (CPU): 96.85 Â± 9.17 ms ğŸ¢ 2.1x slower\n",
      "  PyTorch (GPU): 1.69 Â± 0.00 ms ğŸš€ 27.6x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_matmul(sizes: List[int] = [256, 512, 1024, 2048, 4096]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark matrix multiplication across different sizes and devices.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nMatrix size: {size}x{size}\")\n",
    "        \n",
    "        # NumPy\n",
    "        a_np = np.random.randn(size, size).astype(np.float32)\n",
    "        b_np = np.random.randn(size, size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: np.matmul(a_np, b_np))\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        a_cpu = torch.from_numpy(a_np)\n",
    "        b_cpu = torch.from_numpy(b_np)\n",
    "        torch_cpu_time = benchmark(lambda: torch.matmul(a_cpu, b_cpu))\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU (if available)\n",
    "        if GPU:\n",
    "            a_gpu = a_cpu.to(GPU)\n",
    "            b_gpu = b_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: torch.matmul(a_gpu, b_gpu))\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 1: Matrix Multiplication (A @ B)\")\n",
    "print(\"=\"*60)\n",
    "matmul_results = benchmark_matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 2: Element-wise Operations\n",
    "\n",
    "Many scientific computations involve element-wise operations on large arrays. These are highly parallelizableâ€”perfect for GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 2: Element-wise Operations (sin(x) * exp(-xÂ²))\n",
      "============================================================\n",
      "\n",
      "Array size: 1,000,000 elements\n",
      "  NumPy:         0.84 Â± 0.08 ms\n",
      "  PyTorch (CPU): 0.23 Â± 0.01 ms ğŸš€ 3.6x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 39.0x faster\n",
      "\n",
      "Array size: 10,000,000 elements\n",
      "  NumPy:         14.87 Â± 0.44 ms\n",
      "  PyTorch (CPU): 14.76 Â± 0.37 ms ğŸš€ 1.0x faster\n",
      "  PyTorch (GPU): 0.13 Â± 0.00 ms ğŸš€ 113.0x faster\n",
      "\n",
      "Array size: 50,000,000 elements\n",
      "  NumPy:         79.53 Â± 0.51 ms\n",
      "  PyTorch (CPU): 90.68 Â± 1.52 ms ğŸ¢ 1.1x slower\n",
      "  PyTorch (GPU): 1.49 Â± 0.00 ms ğŸš€ 53.5x faster\n",
      "\n",
      "Array size: 100,000,000 elements\n",
      "  NumPy:         157.20 Â± 0.34 ms\n",
      "  PyTorch (CPU): 185.84 Â± 1.74 ms ğŸ¢ 1.2x slower\n",
      "  PyTorch (GPU): 2.98 Â± 0.00 ms ğŸš€ 52.7x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_elementwise(sizes: List[int] = [1_000_000, 10_000_000, 50_000_000, 100_000_000]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark element-wise operations: sin, exp, and compound expressions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nArray size: {size:,} elements\")\n",
    "        \n",
    "        # Test expression: sin(x) * exp(-x^2) - common in physics/signal processing\n",
    "        \n",
    "        # NumPy\n",
    "        x_np = np.random.randn(size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: np.sin(x_np) * np.exp(-x_np**2))\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', 'operation': 'sin*exp', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        x_cpu = torch.from_numpy(x_np)\n",
    "        torch_cpu_time = benchmark(lambda: torch.sin(x_cpu) * torch.exp(-x_cpu**2))\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', 'operation': 'sin*exp', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            x_gpu = x_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: torch.sin(x_gpu) * torch.exp(-x_gpu**2))\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', 'operation': 'sin*exp', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 2: Element-wise Operations (sin(x) * exp(-xÂ²))\")\n",
    "print(\"=\"*60)\n",
    "elementwise_results = benchmark_elementwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 3: Linear Algebra (SVD, Eigendecomposition)\n",
    "\n",
    "Linear algebra operations are critical for scientific computing, dimensionality reduction, and solving systems of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 3: Linear Algebra (SVD, Eigendecomposition)\n",
      "============================================================\n",
      "\n",
      "========================================\n",
      "Matrix size: 256x256\n",
      "\n",
      "  SVD:\n",
      "    NumPy:         8.25 Â± 0.24 ms\n",
      "    PyTorch (CPU): 4.14 Â± 0.02 ms ğŸš€ 2.0x faster\n",
      "    PyTorch (GPU): 6.09 Â± 0.01 ms ğŸš€ 1.4x faster\n",
      "\n",
      "  Eigendecomposition (symmetric):\n",
      "    NumPy:         4.09 Â± 0.01 ms\n",
      "    PyTorch (CPU): 1.39 Â± 0.01 ms ğŸš€ 2.9x faster\n",
      "    PyTorch (GPU): 4.03 Â± 0.03 ms ğŸš€ 1.0x faster\n",
      "\n",
      "========================================\n",
      "Matrix size: 512x512\n",
      "\n",
      "  SVD:\n",
      "    NumPy:         43.58 Â± 0.21 ms\n",
      "    PyTorch (CPU): 15.64 Â± 0.06 ms ğŸš€ 2.8x faster\n",
      "    PyTorch (GPU): 15.87 Â± 0.03 ms ğŸš€ 2.7x faster\n",
      "\n",
      "  Eigendecomposition (symmetric):\n",
      "    NumPy:         17.83 Â± 0.02 ms\n",
      "    PyTorch (CPU): 7.84 Â± 2.65 ms ğŸš€ 2.3x faster\n",
      "    PyTorch (GPU): 11.13 Â± 0.01 ms ğŸš€ 1.6x faster\n",
      "\n",
      "========================================\n",
      "Matrix size: 1024x1024\n",
      "\n",
      "  SVD:\n",
      "    NumPy:         202.51 Â± 1.28 ms\n",
      "    PyTorch (CPU): 71.98 Â± 0.14 ms ğŸš€ 2.8x faster\n",
      "    PyTorch (GPU): 48.99 Â± 0.01 ms ğŸš€ 4.1x faster\n",
      "\n",
      "  Eigendecomposition (symmetric):\n",
      "    NumPy:         75.98 Â± 0.19 ms\n",
      "    PyTorch (CPU): 20.14 Â± 0.06 ms ğŸš€ 3.8x faster\n",
      "    PyTorch (GPU): 8.35 Â± 0.01 ms ğŸš€ 9.1x faster\n",
      "\n",
      "========================================\n",
      "Matrix size: 2048x2048\n",
      "\n",
      "  SVD:\n",
      "    NumPy:         997.10 Â± 20.54 ms\n",
      "    PyTorch (CPU): 379.65 Â± 3.62 ms ğŸš€ 2.6x faster\n",
      "    PyTorch (GPU): 167.88 Â± 0.02 ms ğŸš€ 5.9x faster\n",
      "\n",
      "  Eigendecomposition (symmetric):\n",
      "    NumPy:         390.46 Â± 3.62 ms\n",
      "    PyTorch (CPU): 104.40 Â± 0.07 ms ğŸš€ 3.7x faster\n",
      "    PyTorch (GPU): 19.17 Â± 0.06 ms ğŸš€ 20.4x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_linalg(sizes: List[int] = [256, 512, 1024, 2048]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark linear algebra operations: SVD and eigendecomposition.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Matrix size: {size}x{size}\")\n",
    "        \n",
    "        # --- SVD ---\n",
    "        print(f\"\\n  SVD:\")\n",
    "        \n",
    "        # NumPy\n",
    "        a_np = np.random.randn(size, size).astype(np.float32)\n",
    "        numpy_svd_time = benchmark(lambda: np.linalg.svd(a_np), n_runs=5)\n",
    "        print(f\"    NumPy:         {numpy_svd_time['mean']:.2f} Â± {numpy_svd_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', 'operation': 'SVD', **numpy_svd_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        a_cpu = torch.from_numpy(a_np)\n",
    "        torch_cpu_svd_time = benchmark(lambda: torch.linalg.svd(a_cpu), n_runs=5)\n",
    "        print(f\"    PyTorch (CPU): {torch_cpu_svd_time['mean']:.2f} Â± {torch_cpu_svd_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_svd_time['mean'], torch_cpu_svd_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', 'operation': 'SVD', **torch_cpu_svd_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            a_gpu = a_cpu.to(GPU)\n",
    "            torch_gpu_svd_time = benchmark(lambda: torch.linalg.svd(a_gpu), n_runs=5)\n",
    "            print(f\"    PyTorch (GPU): {torch_gpu_svd_time['mean']:.2f} Â± {torch_gpu_svd_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_svd_time['mean'], torch_gpu_svd_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', 'operation': 'SVD', **torch_gpu_svd_time})\n",
    "        \n",
    "        # --- Eigendecomposition (symmetric matrix) ---\n",
    "        print(f\"\\n  Eigendecomposition (symmetric):\")\n",
    "        \n",
    "        # Create symmetric matrix\n",
    "        sym_np = (a_np + a_np.T) / 2\n",
    "        \n",
    "        # NumPy\n",
    "        numpy_eig_time = benchmark(lambda: np.linalg.eigh(sym_np), n_runs=5)\n",
    "        print(f\"    NumPy:         {numpy_eig_time['mean']:.2f} Â± {numpy_eig_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', 'operation': 'Eigh', **numpy_eig_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        sym_cpu = torch.from_numpy(sym_np)\n",
    "        torch_cpu_eig_time = benchmark(lambda: torch.linalg.eigh(sym_cpu), n_runs=5)\n",
    "        print(f\"    PyTorch (CPU): {torch_cpu_eig_time['mean']:.2f} Â± {torch_cpu_eig_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_eig_time['mean'], torch_cpu_eig_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', 'operation': 'Eigh', **torch_cpu_eig_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            sym_gpu = sym_cpu.to(GPU)\n",
    "            torch_gpu_eig_time = benchmark(lambda: torch.linalg.eigh(sym_gpu), n_runs=5)\n",
    "            print(f\"    PyTorch (GPU): {torch_gpu_eig_time['mean']:.2f} Â± {torch_gpu_eig_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_eig_time['mean'], torch_gpu_eig_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', 'operation': 'Eigh', **torch_gpu_eig_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 3: Linear Algebra (SVD, Eigendecomposition)\")\n",
    "print(\"=\"*60)\n",
    "linalg_results = benchmark_linalg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 4: Reductions (Sum, Mean, Std)\n",
    "\n",
    "Aggregation operations over large arraysâ€”essential for statistics and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 4: Reductions (sum, mean, std)\n",
      "============================================================\n",
      "\n",
      "Array size: 1,000,000 elements\n",
      "  NumPy:         0.67 Â± 0.01 ms\n",
      "  PyTorch (CPU): 0.10 Â± 0.00 ms ğŸš€ 6.8x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 27.6x faster\n",
      "\n",
      "Array size: 10,000,000 elements\n",
      "  NumPy:         8.84 Â± 0.53 ms\n",
      "  PyTorch (CPU): 0.53 Â± 0.01 ms ğŸš€ 16.6x faster\n",
      "  PyTorch (GPU): 0.04 Â± 0.00 ms ğŸš€ 223.3x faster\n",
      "\n",
      "Array size: 100,000,000 elements\n",
      "  NumPy:         89.82 Â± 0.45 ms\n",
      "  PyTorch (CPU): 24.90 Â± 0.12 ms ğŸš€ 3.6x faster\n",
      "  PyTorch (GPU): 0.75 Â± 0.00 ms ğŸš€ 120.2x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_reductions(sizes: List[int] = [1_000_000, 10_000_000, 100_000_000]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark reduction operations: sum, mean, std.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nArray size: {size:,} elements\")\n",
    "        \n",
    "        # NumPy\n",
    "        x_np = np.random.randn(size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: (np.sum(x_np), np.mean(x_np), np.std(x_np)))\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        x_cpu = torch.from_numpy(x_np)\n",
    "        torch_cpu_time = benchmark(lambda: (torch.sum(x_cpu), torch.mean(x_cpu), torch.std(x_cpu)))\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            x_gpu = x_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: (torch.sum(x_gpu), torch.mean(x_gpu), torch.std(x_gpu)))\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 4: Reductions (sum, mean, std)\")\n",
    "print(\"=\"*60)\n",
    "reduction_results = benchmark_reductions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 5: FFT (Fast Fourier Transform)\n",
    "\n",
    "FFT is fundamental for signal processing, spectral analysis, and solving PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 5: Fast Fourier Transform\n",
      "============================================================\n",
      "\n",
      "--- 1D FFT ---\n",
      "\n",
      "Signal length: 65,536\n",
      "  NumPy:         0.49 Â± 0.00 ms\n",
      "  PyTorch (CPU): 0.23 Â± 0.02 ms ğŸš€ 2.1x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 24.5x faster\n",
      "\n",
      "Signal length: 262,144\n",
      "  NumPy:         2.07 Â± 0.00 ms\n",
      "  PyTorch (CPU): 0.51 Â± 0.00 ms ğŸš€ 4.0x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 89.1x faster\n",
      "\n",
      "Signal length: 1,048,576\n",
      "  NumPy:         16.10 Â± 0.18 ms\n",
      "  PyTorch (CPU): 0.93 Â± 0.01 ms ğŸš€ 17.3x faster\n",
      "  PyTorch (GPU): 0.03 Â± 0.00 ms ğŸš€ 598.6x faster\n",
      "\n",
      "Signal length: 4,194,304\n",
      "  NumPy:         88.73 Â± 4.44 ms\n",
      "  PyTorch (CPU): 18.95 Â± 0.33 ms ğŸš€ 4.7x faster\n",
      "  PyTorch (GPU): 0.05 Â± 0.00 ms ğŸš€ 1743.2x faster\n",
      "\n",
      "--- 2D FFT ---\n",
      "\n",
      "Image size: 512x512\n",
      "  NumPy:         2.44 Â± 0.41 ms\n",
      "  PyTorch (CPU): 0.16 Â± 0.00 ms ğŸš€ 15.1x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 160.2x faster\n",
      "\n",
      "Image size: 1024x1024\n",
      "  NumPy:         12.67 Â± 0.09 ms\n",
      "  PyTorch (CPU): 0.42 Â± 0.01 ms ğŸš€ 30.4x faster\n",
      "  PyTorch (GPU): 0.02 Â± 0.00 ms ğŸš€ 591.1x faster\n",
      "\n",
      "Image size: 2048x2048\n",
      "  NumPy:         88.67 Â± 0.31 ms\n",
      "  PyTorch (CPU): 5.37 Â± 0.39 ms ğŸš€ 16.5x faster\n",
      "  PyTorch (GPU): 0.05 Â± 0.00 ms ğŸš€ 1705.5x faster\n",
      "\n",
      "Image size: 4096x4096\n",
      "  NumPy:         1032.31 Â± 1.29 ms\n",
      "  PyTorch (CPU): 30.89 Â± 0.35 ms ğŸš€ 33.4x faster\n",
      "  PyTorch (GPU): 0.60 Â± 0.00 ms ğŸš€ 1714.1x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_fft(sizes: List[int] = [2**16, 2**18, 2**20, 2**22]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark 1D and 2D FFT operations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n--- 1D FFT ---\")\n",
    "    for size in sizes:\n",
    "        print(f\"\\nSignal length: {size:,}\")\n",
    "        \n",
    "        # NumPy\n",
    "        x_np = np.random.randn(size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: np.fft.fft(x_np))\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', 'operation': 'FFT1D', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        x_cpu = torch.from_numpy(x_np)\n",
    "        torch_cpu_time = benchmark(lambda: torch.fft.fft(x_cpu))\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', 'operation': 'FFT1D', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            x_gpu = x_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: torch.fft.fft(x_gpu))\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', 'operation': 'FFT1D', **torch_gpu_time})\n",
    "    \n",
    "    print(\"\\n--- 2D FFT ---\")\n",
    "    sizes_2d = [512, 1024, 2048, 4096]\n",
    "    for size in sizes_2d:\n",
    "        print(f\"\\nImage size: {size}x{size}\")\n",
    "        \n",
    "        # NumPy\n",
    "        x_np = np.random.randn(size, size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: np.fft.fft2(x_np))\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', 'operation': 'FFT2D', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        x_cpu = torch.from_numpy(x_np)\n",
    "        torch_cpu_time = benchmark(lambda: torch.fft.fft2(x_cpu))\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', 'operation': 'FFT2D', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            x_gpu = x_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: torch.fft.fft2(x_gpu))\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', 'operation': 'FFT2D', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 5: Fast Fourier Transform\")\n",
    "print(\"=\"*60)\n",
    "fft_results = benchmark_fft()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 6: Real-World Simulation â€” Heat Equation (PDE Solver)\n",
    "\n",
    "This is where the PyTorch advantage becomes compelling. Let's solve the 2D heat equation using finite differencesâ€”a common task in scientific computing.\n",
    "\n",
    "The key insight: **the exact same code runs on CPU or GPU with a single `.to(device)` call.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 6: Heat Equation PDE Solver (Real-World Simulation)\n",
      "============================================================\n",
      "\n",
      "Grid size: 256x256, 100 time steps\n",
      "  NumPy:         6.08 Â± 0.39 ms\n",
      "  PyTorch (CPU): 7.51 Â± 0.04 ms ğŸ¢ 1.2x slower\n",
      "  PyTorch (GPU): 3.11 Â± 0.01 ms ğŸš€ 2.0x faster\n",
      "\n",
      "Grid size: 512x512, 100 time steps\n",
      "  NumPy:         19.65 Â± 0.02 ms\n",
      "  PyTorch (CPU): 8.45 Â± 0.23 ms ğŸš€ 2.3x faster\n",
      "  PyTorch (GPU): 3.25 Â± 0.05 ms ğŸš€ 6.0x faster\n",
      "\n",
      "Grid size: 1024x1024, 100 time steps\n",
      "  NumPy:         73.21 Â± 3.31 ms\n",
      "  PyTorch (CPU): 14.80 Â± 0.99 ms ğŸš€ 4.9x faster\n",
      "  PyTorch (GPU): 4.11 Â± 0.00 ms ğŸš€ 17.8x faster\n",
      "\n",
      "Grid size: 2048x2048, 100 time steps\n",
      "  NumPy:         815.62 Â± 6.00 ms\n",
      "  PyTorch (CPU): 370.80 Â± 0.67 ms ğŸš€ 2.2x faster\n",
      "  PyTorch (GPU): 9.14 Â± 0.04 ms ğŸš€ 89.3x faster\n"
     ]
    }
   ],
   "source": [
    "def heat_equation_numpy(u: np.ndarray, alpha: float, dx: float, dt: float, steps: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve 2D heat equation using finite differences.\n",
    "    âˆ‚u/âˆ‚t = Î±âˆ‡Â²u\n",
    "    \"\"\"\n",
    "    factor = alpha * dt / (dx ** 2)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Laplacian using finite differences\n",
    "        laplacian = (\n",
    "            np.roll(u, 1, axis=0) + np.roll(u, -1, axis=0) +\n",
    "            np.roll(u, 1, axis=1) + np.roll(u, -1, axis=1) - 4 * u\n",
    "        )\n",
    "        u = u + factor * laplacian\n",
    "    \n",
    "    return u\n",
    "\n",
    "\n",
    "def heat_equation_pytorch(u: torch.Tensor, alpha: float, dx: float, dt: float, steps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Solve 2D heat equation using finite differences.\n",
    "    SAME LOGIC as NumPy version â€” but runs on ANY device.\n",
    "    \"\"\"\n",
    "    factor = alpha * dt / (dx ** 2)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Laplacian using finite differences\n",
    "        laplacian = (\n",
    "            torch.roll(u, 1, dims=0) + torch.roll(u, -1, dims=0) +\n",
    "            torch.roll(u, 1, dims=1) + torch.roll(u, -1, dims=1) - 4 * u\n",
    "        )\n",
    "        u = u + factor * laplacian\n",
    "    \n",
    "    return u\n",
    "\n",
    "\n",
    "def benchmark_heat_equation(grid_sizes: List[int] = [256, 512, 1024, 2048], \n",
    "                            n_steps: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark the heat equation solver.\"\"\"\n",
    "    results = []\n",
    "    alpha = 0.1  # Thermal diffusivity\n",
    "    dx = 0.01    # Grid spacing\n",
    "    dt = 0.0001  # Time step\n",
    "    \n",
    "    for size in grid_sizes:\n",
    "        print(f\"\\nGrid size: {size}x{size}, {n_steps} time steps\")\n",
    "        \n",
    "        # Initial condition: hot spot in center\n",
    "        u_np = np.zeros((size, size), dtype=np.float32)\n",
    "        center = size // 2\n",
    "        u_np[center-10:center+10, center-10:center+10] = 100.0\n",
    "        \n",
    "        # NumPy\n",
    "        numpy_time = benchmark(lambda: heat_equation_numpy(u_np.copy(), alpha, dx, dt, n_steps), n_runs=5)\n",
    "        print(f\"  NumPy:         {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'library': 'NumPy', 'device': 'CPU', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        u_cpu = torch.from_numpy(u_np)\n",
    "        torch_cpu_time = benchmark(lambda: heat_equation_pytorch(u_cpu.clone(), alpha, dx, dt, n_steps), n_runs=5)\n",
    "        print(f\"  PyTorch (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'library': 'PyTorch', 'device': 'CPU', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU â€” SAME CODE, just .to(GPU)\n",
    "        if GPU:\n",
    "            u_gpu = u_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: heat_equation_pytorch(u_gpu.clone(), alpha, dx, dt, n_steps), n_runs=5)\n",
    "            print(f\"  PyTorch (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'library': 'PyTorch', 'device': 'GPU', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 6: Heat Equation PDE Solver (Real-World Simulation)\")\n",
    "print(\"=\"*60)\n",
    "heat_results = benchmark_heat_equation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 7: Batched Operations\n",
    "\n",
    "PyTorch excels at batched computationsâ€”processing many samples simultaneously. This is common in Monte Carlo simulations, ensemble methods, and (of course) ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 7: Batched Matrix Multiplication\n",
      "============================================================\n",
      "\n",
      "Batch size: 64, Matrix size: 256x256\n",
      "  NumPy (einsum):  328.09 Â± 0.11 ms\n",
      "  PyTorch (CPU):   1.67 Â± 0.00 ms ğŸš€ 196.3x faster\n",
      "  PyTorch (GPU):   0.06 Â± 0.00 ms ğŸš€ 5911.9x faster\n",
      "\n",
      "Batch size: 256, Matrix size: 256x256\n",
      "  NumPy (einsum):  1318.73 Â± 2.27 ms\n",
      "  PyTorch (CPU):   8.00 Â± 0.16 ms ğŸš€ 164.8x faster\n",
      "  PyTorch (GPU):   0.21 Â± 0.00 ms ğŸš€ 6423.6x faster\n",
      "\n",
      "Batch size: 1024, Matrix size: 256x256\n",
      "  NumPy (einsum):  5261.58 Â± 8.65 ms\n",
      "  PyTorch (CPU):   39.07 Â± 2.85 ms ğŸš€ 134.7x faster\n",
      "  PyTorch (GPU):   0.80 Â± 0.03 ms ğŸš€ 6540.9x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_batched_matmul(batch_sizes: List[int] = [64, 256, 1024], \n",
    "                              matrix_size: int = 256) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark batched matrix multiplication.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nBatch size: {batch_size}, Matrix size: {matrix_size}x{matrix_size}\")\n",
    "        \n",
    "        # NumPy â€” have to use einsum or loop\n",
    "        a_np = np.random.randn(batch_size, matrix_size, matrix_size).astype(np.float32)\n",
    "        b_np = np.random.randn(batch_size, matrix_size, matrix_size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: np.einsum('bij,bjk->bik', a_np, b_np), n_runs=5)\n",
    "        print(f\"  NumPy (einsum):  {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'batch_size': batch_size, 'library': 'NumPy', 'device': 'CPU', **numpy_time})\n",
    "        \n",
    "        # PyTorch CPU â€” native batched matmul\n",
    "        a_cpu = torch.from_numpy(a_np)\n",
    "        b_cpu = torch.from_numpy(b_np)\n",
    "        torch_cpu_time = benchmark(lambda: torch.bmm(a_cpu, b_cpu), n_runs=5)\n",
    "        print(f\"  PyTorch (CPU):   {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'batch_size': batch_size, 'library': 'PyTorch', 'device': 'CPU', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch GPU\n",
    "        if GPU:\n",
    "            a_gpu = a_cpu.to(GPU)\n",
    "            b_gpu = b_cpu.to(GPU)\n",
    "            torch_gpu_time = benchmark(lambda: torch.bmm(a_gpu, b_gpu), n_runs=5)\n",
    "            print(f\"  PyTorch (GPU):   {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'batch_size': batch_size, 'library': 'PyTorch', 'device': 'GPU', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 7: Batched Matrix Multiplication\")\n",
    "print(\"=\"*60)\n",
    "batched_results = benchmark_batched_matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Benchmark 8: Automatic Differentiation â€” The Hidden Superpower\n",
    "\n",
    "This is where PyTorch fundamentally changes what's possible. You get **exact gradients for free** on any computation.\n",
    "\n",
    "NumPy? You'd have to derive and implement gradients manually, or use finite differences (slow and numerically unstable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BENCHMARK 8: Gradient Computation\n",
      "NumPy: Finite differences (slow, approximate)\n",
      "PyTorch: Automatic differentiation (fast, exact)\n",
      "============================================================\n",
      "\n",
      "Matrix size: 16x16 (256 gradient components)\n",
      "  NumPy (finite diff): 3.20 Â± 0.02 ms\n",
      "  PyTorch autograd (CPU): 0.06 Â± 0.00 ms ğŸš€ 55.7x faster\n",
      "  PyTorch autograd (GPU): 0.14 Â± 0.01 ms ğŸš€ 22.7x faster\n",
      "\n",
      "Matrix size: 32x32 (1,024 gradient components)\n",
      "  NumPy (finite diff): 17.23 Â± 0.05 ms\n",
      "  PyTorch autograd (CPU): 0.08 Â± 0.01 ms ğŸš€ 210.1x faster\n",
      "  PyTorch autograd (GPU): 0.17 Â± 0.00 ms ğŸš€ 101.8x faster\n",
      "\n",
      "Matrix size: 64x64 (4,096 gradient components)\n",
      "  NumPy (finite diff): 90.71 Â± 0.21 ms\n",
      "  PyTorch autograd (CPU): 0.09 Â± 0.01 ms ğŸš€ 1026.5x faster\n",
      "  PyTorch autograd (GPU): 0.14 Â± 0.00 ms ğŸš€ 656.7x faster\n",
      "\n",
      "Matrix size: 128x128 (16,384 gradient components)\n",
      "  NumPy (finite diff): 1125.49 Â± 0.32 ms\n",
      "  PyTorch autograd (CPU): 0.15 Â± 0.01 ms ğŸš€ 7632.9x faster\n",
      "  PyTorch autograd (GPU): 0.15 Â± 0.00 ms ğŸš€ 7447.9x faster\n"
     ]
    }
   ],
   "source": [
    "def complex_function_numpy(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"A complex function that would be tedious to differentiate by hand.\"\"\"\n",
    "    return np.sum(np.sin(x @ x.T) * np.exp(-np.abs(x).sum(axis=1, keepdims=True)))\n",
    "\n",
    "\n",
    "def numerical_gradient_numpy(x: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n",
    "    \"\"\"Compute gradient using finite differences â€” slow and approximate.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x_plus = x.copy()\n",
    "            x_plus[i, j] += eps\n",
    "            x_minus = x.copy()\n",
    "            x_minus[i, j] -= eps\n",
    "            grad[i, j] = (complex_function_numpy(x_plus) - complex_function_numpy(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def complex_function_pytorch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Same function in PyTorch.\"\"\"\n",
    "    return torch.sum(torch.sin(x @ x.T) * torch.exp(-torch.abs(x).sum(dim=1, keepdim=True)))\n",
    "\n",
    "\n",
    "def benchmark_autograd(sizes: List[int] = [16, 32, 64, 128]) -> pd.DataFrame:\n",
    "    \"\"\"Benchmark gradient computation: NumPy finite differences vs PyTorch autograd.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"\\nMatrix size: {size}x{size} ({size*size:,} gradient components)\")\n",
    "        \n",
    "        # NumPy finite differences\n",
    "        x_np = np.random.randn(size, size).astype(np.float32)\n",
    "        numpy_time = benchmark(lambda: numerical_gradient_numpy(x_np), n_runs=3, n_warmup=1)\n",
    "        print(f\"  NumPy (finite diff): {numpy_time['mean']:.2f} Â± {numpy_time['std']:.2f} ms\")\n",
    "        results.append({'size': size, 'method': 'NumPy finite diff', 'device': 'CPU', **numpy_time})\n",
    "        \n",
    "        # PyTorch autograd CPU\n",
    "        x_cpu = torch.from_numpy(x_np).requires_grad_(True)\n",
    "        def pytorch_grad_cpu():\n",
    "            x_cpu.grad = None\n",
    "            y = complex_function_pytorch(x_cpu)\n",
    "            y.backward()\n",
    "            return x_cpu.grad\n",
    "        torch_cpu_time = benchmark(pytorch_grad_cpu, n_runs=10)\n",
    "        print(f\"  PyTorch autograd (CPU): {torch_cpu_time['mean']:.2f} Â± {torch_cpu_time['std']:.2f} ms \"\n",
    "              f\"{format_speedup(numpy_time['mean'], torch_cpu_time['mean'])}\")\n",
    "        results.append({'size': size, 'method': 'PyTorch autograd', 'device': 'CPU', **torch_cpu_time})\n",
    "        \n",
    "        # PyTorch autograd GPU\n",
    "        if GPU:\n",
    "            x_gpu = torch.from_numpy(x_np).to(GPU).requires_grad_(True)\n",
    "            def pytorch_grad_gpu():\n",
    "                x_gpu.grad = None\n",
    "                y = complex_function_pytorch(x_gpu)\n",
    "                y.backward()\n",
    "                return x_gpu.grad\n",
    "            torch_gpu_time = benchmark(pytorch_grad_gpu, n_runs=10)\n",
    "            print(f\"  PyTorch autograd (GPU): {torch_gpu_time['mean']:.2f} Â± {torch_gpu_time['std']:.2f} ms \"\n",
    "                  f\"{format_speedup(numpy_time['mean'], torch_gpu_time['mean'])}\")\n",
    "            results.append({'size': size, 'method': 'PyTorch autograd', 'device': 'GPU', **torch_gpu_time})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK 8: Gradient Computation\")\n",
    "print(\"NumPy: Finite differences (slow, approximate)\")\n",
    "print(\"PyTorch: Automatic differentiation (fast, exact)\")\n",
    "print(\"=\"*60)\n",
    "autograd_results = benchmark_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: The PyTorch Advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SUMMARY: Why PyTorch for General Numerical Computing?\n",
      "======================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  WRITE ONCE, ACCELERATE ANYWHERE                                    â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  The same PyTorch code runs on:                                     â”‚\n",
      "â”‚    â€¢ CPU (your laptop)                                              â”‚\n",
      "â”‚    â€¢ NVIDIA GPU (CUDA)                                              â”‚\n",
      "â”‚    â€¢ Apple Silicon (MPS)                                            â”‚\n",
      "â”‚    â€¢ AMD GPU (ROCm)                                                 â”‚\n",
      "â”‚    â€¢ Intel GPU (XPU)                                                â”‚\n",
      "â”‚    â€¢ Google TPU                                                     â”‚\n",
      "â”‚    â€¢ Future accelerators (as PyTorch adds support)                  â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  NumPy code? CPU only. Forever.                                     â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  AUTOMATIC DIFFERENTIATION                                          â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  PyTorch: y.backward() â†’ exact gradients, any computation           â”‚\n",
      "â”‚  NumPy:   Manual derivation or slow finite differences              â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  This enables:                                                      â”‚\n",
      "â”‚    â€¢ Optimization problems (gradient descent for free)              â”‚\n",
      "â”‚    â€¢ Sensitivity analysis                                           â”‚\n",
      "â”‚    â€¢ Physics-informed computing                                     â”‚\n",
      "â”‚    â€¢ Inverse problems                                               â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  FUTURE-PROOF                                                       â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  PyTorch is actively developed with massive resources:              â”‚\n",
      "â”‚    â€¢ Better kernels every release                                   â”‚\n",
      "â”‚    â€¢ New hardware support                                           â”‚\n",
      "â”‚    â€¢ torch.compile() for automatic optimization                     â”‚\n",
      "â”‚    â€¢ Quantization, sparsity, mixed precision                        â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  Your code today will be faster tomorrow â€” automatically.           â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  ML-READY                                                           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚                                                                     â”‚\n",
      "â”‚  When you need to add ML to your pipeline:                          â”‚\n",
      "â”‚    â€¢ Data already in tensors                                        â”‚\n",
      "â”‚    â€¢ Same device (no CPUâ†”GPU transfers)                             â”‚\n",
      "â”‚    â€¢ Seamless integration with models                               â”‚\n",
      "â”‚                                                                     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: Why PyTorch for General Numerical Computing?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  WRITE ONCE, ACCELERATE ANYWHERE                                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  The same PyTorch code runs on:                                     â”‚\n",
    "â”‚    â€¢ CPU (your laptop)                                              â”‚\n",
    "â”‚    â€¢ NVIDIA GPU (CUDA)                                              â”‚\n",
    "â”‚    â€¢ Apple Silicon (MPS)                                            â”‚\n",
    "â”‚    â€¢ AMD GPU (ROCm)                                                 â”‚\n",
    "â”‚    â€¢ Intel GPU (XPU)                                                â”‚\n",
    "â”‚    â€¢ Google TPU                                                     â”‚\n",
    "â”‚    â€¢ Future accelerators (as PyTorch adds support)                  â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  NumPy code? CPU only. Forever.                                     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  AUTOMATIC DIFFERENTIATION                                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PyTorch: y.backward() â†’ exact gradients, any computation           â”‚\n",
    "â”‚  NumPy:   Manual derivation or slow finite differences              â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  This enables:                                                      â”‚\n",
    "â”‚    â€¢ Optimization problems (gradient descent for free)              â”‚\n",
    "â”‚    â€¢ Sensitivity analysis                                           â”‚\n",
    "â”‚    â€¢ Physics-informed computing                                     â”‚\n",
    "â”‚    â€¢ Inverse problems                                               â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  FUTURE-PROOF                                                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PyTorch is actively developed with massive resources:              â”‚\n",
    "â”‚    â€¢ Better kernels every release                                   â”‚\n",
    "â”‚    â€¢ New hardware support                                           â”‚\n",
    "â”‚    â€¢ torch.compile() for automatic optimization                     â”‚\n",
    "â”‚    â€¢ Quantization, sparsity, mixed precision                        â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  Your code today will be faster tomorrow â€” automatically.           â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  ML-READY                                                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  When you need to add ML to your pipeline:                          â”‚\n",
    "â”‚    â€¢ Data already in tensors                                        â”‚\n",
    "â”‚    â€¢ Same device (no CPUâ†”GPU transfers)                             â”‚\n",
    "â”‚    â€¢ Seamless integration with models                               â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: torch.compile() â€” The Future is Now\n",
    "\n",
    "PyTorch 2.0+ includes `torch.compile()`, which can automatically optimize your code with minimal changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile() is available!\n",
      "\n",
      "Benchmarking on 10,000,000 elements:\n",
      "  Eager mode:    12.17 ms\n",
      "  Compiled mode: 0.32 ms ğŸš€ 37.8x faster\n",
      "  Eager (GPU):   0.06 ms\n",
      "  Compiled (GPU): 0.03 ms ğŸš€ 2.4x faster\n"
     ]
    }
   ],
   "source": [
    "# Check if torch.compile is available (PyTorch 2.0+)\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"torch.compile() is available!\")\n",
    "    \n",
    "    def demo_function(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"A function with multiple operations that can be fused.\"\"\"\n",
    "        x = torch.sin(x)\n",
    "        x = x * 2\n",
    "        x = torch.relu(x)\n",
    "        x = x + 1\n",
    "        return x.sum()\n",
    "    \n",
    "    # Compile the function\n",
    "    compiled_function = torch.compile(demo_function)\n",
    "    \n",
    "    size = 10_000_000\n",
    "    x = torch.randn(size)\n",
    "    \n",
    "    print(f\"\\nBenchmarking on {size:,} elements:\")\n",
    "    \n",
    "    # Eager mode\n",
    "    eager_time = benchmark(lambda: demo_function(x))\n",
    "    print(f\"  Eager mode:    {eager_time['mean']:.2f} ms\")\n",
    "    \n",
    "    # Compiled mode (first run includes compilation)\n",
    "    compiled_time = benchmark(lambda: compiled_function(x), n_warmup=5)  # Extra warmup for compilation\n",
    "    print(f\"  Compiled mode: {compiled_time['mean']:.2f} ms {format_speedup(eager_time['mean'], compiled_time['mean'])}\")\n",
    "    \n",
    "    if GPU:\n",
    "        x_gpu = x.to(GPU)\n",
    "        compiled_function_gpu = torch.compile(demo_function)\n",
    "        \n",
    "        eager_gpu_time = benchmark(lambda: demo_function(x_gpu))\n",
    "        print(f\"  Eager (GPU):   {eager_gpu_time['mean']:.2f} ms\")\n",
    "        \n",
    "        compiled_gpu_time = benchmark(lambda: compiled_function_gpu(x_gpu), n_warmup=5)\n",
    "        print(f\"  Compiled (GPU): {compiled_gpu_time['mean']:.2f} ms {format_speedup(eager_gpu_time['mean'], compiled_gpu_time['mean'])}\")\n",
    "else:\n",
    "    print(\"torch.compile() requires PyTorch 2.0+\")\n",
    "    print(f\"Current version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "PyTorch isn't just for machine learning anymore. It's a **general-purpose numerical computing library** that happens to also be the best ML framework.\n",
    "\n",
    "By writing your numerical code in PyTorch today, you get:\n",
    "\n",
    "1. **Immediate GPU acceleration** â€” just add `.to('cuda')`\n",
    "2. **Automatic differentiation** â€” gradients for any computation\n",
    "3. **Future performance gains** â€” as PyTorch improves, your code gets faster\n",
    "4. **ML integration** â€” when you need it, your data is ready\n",
    "\n",
    "The question isn't \"should I learn PyTorch for ML?\" â€” it's \"why am I still using NumPy for heavy computation?\"\n",
    "\n",
    "---\n",
    "\n",
    "*DÃ¡vid Isztl | isztld.com*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313-torch2.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
